# Note d'architecture technique
## Projet ETL Open Food Facts

---

## üìã Sommaire

1. [Vue d'ensemble](#vue-densemble)
2. [Choix techniques](#choix-techniques)
3. [Architecture m√©daill√©e](#architecture-m√©daill√©e)
4. [Sch√©ma de donn√©es Gold](#sch√©ma-de-donn√©es-gold)
5. [Strat√©gie de chargement](#strat√©gie-de-chargement)
6. [Performance et scalabilit√©](#performance-et-scalabilit√©)
7. [S√©curit√© et gouvernance](#s√©curit√©-et-gouvernance)

---

## üéØ Vue d'ensemble

### Contexte du projet

**Probl√©matique** : Les donn√©es Open Food Facts sont volumineuses (2.3M+ produits), non structur√©es (215 colonnes disparates) et de qualit√© variable. Il est n√©cessaire de les transformer en un datamart analytique exploitable pour des analyses nutritionnelles.

**Objectifs** :
1. **Ing√©rer** ~400k produits depuis un export CSV
2. **Nettoyer** et valider les donn√©es (normalisation, d√©duplication, validation)
3. **Mod√©liser** en sch√©ma en √©toile pour requ√™tes OLAP
4. **Optimiser** les performances de requ√™tage (indexes, partitionnement)

### Architecture cible

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         SOURCE LAYER                            ‚îÇ
‚îÇ  Open Food Facts CSV Export (Tab-separated, UTF-8)              ‚îÇ
‚îÇ  ‚îî‚îÄ donnees_echantillon.csv (418,676 lignes √ó 215 colonnes)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì (Spark Read CSV)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       BRONZE LAYER (Spark)                      ‚îÇ
‚îÇ  DataFrame en m√©moire (donn√©es brutes, aucune transformation)   ‚îÇ
‚îÇ  ‚îî‚îÄ bronze_df (PySpark DataFrame, ~120 MB)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì (Transformations PySpark)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SILVER LAYER (MySQL InnoDB)                  ‚îÇ
‚îÇ  Table relationnelle normalis√©e et nettoy√©e                     ‚îÇ
‚îÇ  ‚îî‚îÄ silver_products (418,651 lignes √ó 17 colonnes, ~45 MB)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì (Spark ‚Üí MySQL JDBC)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     GOLD LAYER (MySQL InnoDB)                   ‚îÇ
‚îÇ  Datamart analytique - Star Schema                             ‚îÇ
‚îÇ  ‚îú‚îÄ dim_time (50,000 dates uniques)                            ‚îÇ
‚îÇ  ‚îú‚îÄ dim_brand (45,123 marques)                                 ‚îÇ
‚îÇ  ‚îú‚îÄ dim_category (8,000 cat√©gories)                            ‚îÇ
‚îÇ  ‚îú‚îÄ dim_country (180 combinaisons)                             ‚îÇ
‚îÇ  ‚îú‚îÄ dim_product (418,651 produits)                             ‚îÇ
‚îÇ  ‚îî‚îÄ fact_nutrition_snapshot (418,651 snapshots)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Choix techniques

### Stack technologique

| Composant | Technologie s√©lectionn√©e | Alternatives √©valu√©es | Justification du choix |
|-----------|--------------------------|----------------------|------------------------|
| **Processing** | Apache Spark 3.5 | Pandas | Volume > 100 MB, parall√©lisme natif, maturit√© |
| **Langage** | Python 3.11 | Java | √âcosyst√®me data science, lisibilit√© |
| **Stockage** | MySQL 8.0 | PostgreSQL | Simplicit√©, compatibilit√© JDBC, support transactionnel |
| **Connecteur** | MySQL Connector/J 8.0.33 | JDBC natif Spark | Driver officiel Oracle, performances optimales |
| **Environnement dev** | Jupyter Notebook | VS Code| Interactivit√©, visualisation en ligne |

### Justification MySQL vs PostgreSQL

| Crit√®re | MySQL | PostgreSQL | Choix retenu |
|---------|-------|------------|--------------|
| Performance SELECT | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | **MySQL** (OLAP l√©ger) |
| JSON natif | ‚≠ê‚≠ê (depuis 8.0) | ‚≠ê‚≠ê‚≠ê | PostgreSQL meilleur, mais MySQL suffisant |
| Int√©gration Spark | ‚≠ê‚≠ê‚≠ê (natif) | ‚≠ê‚≠ê‚≠ê (natif) | √âquivalent |
| Facilit√© setup | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | **MySQL** (plus simple) |
| Co√ªt op√©rationnel | Gratuit (Community) | Gratuit (Open Source) | √âquivalent |

**D√©cision** : MySQL retenu pour sa simplicit√© dans un contexte p√©dagogique

---

## ü•âü•àü•á Architecture m√©daill√©e


### Couche Bronze (Raw Data)

**R√¥le** : Zone de staging des donn√©es brutes

**Caract√©ristiques** :
- Format : DataFrame Spark en m√©moire (non persist√©)
- Sch√©ma : Inf√©r√© automatiquement depuis CSV
- Transformations : **Aucune** (lecture pure)
- Dur√©e de vie : Temporaire (session Spark)

**Impl√©mentation** :
```python
bronze_df = spark.read \
    .option("header", "true") \
    .option("sep", "\t") \
    .option("quote", '"') \
    .option("escape", '"') \
    .option("multiLine", "true") \
    .option("mode", "PERMISSIVE") \  # Tol√©rance erreurs
    .csv(csv_path)
```

**Design pattern** : **Schema-on-read** (pas de validation stricte)

---

### Couche Silver (Clean Data)

**R√¥le** : Zone de donn√©es nettoy√©es et valid√©es

**Caract√©ristiques** :
- Format : Table MySQL InnoDB
- Sch√©ma : D√©fini explicitement (17 colonnes)
- Transformations : Nettoyage, validation, enrichissement
- Dur√©e de vie : Persist√© (remplac√© √† chaque ex√©cution)

**Transformations cl√©s** :
1. **S√©lection de colonnes** : 215 ‚Üí 17 (r√©duction 92%)
2. **Normalisation Unicode** : NFD ‚Üí ASCII
3. **Validation par plages** : Seuils nutritionnels
4. **Enrichissement** : Calcul energy_kj, sodium/sel
5. **D√©duplication** : Par code-barres + date

**Mode d'√©criture** :
```python
silver_final.write.jdbc(
    url=jdbc_url,
    table="silver_products",
    mode="overwrite",  # Remplacement complet
    properties=connection_props
)
```

**Design pattern** : **Data Quality Firewall** (validation stricte en entr√©e)

---

### Couche Gold (Analytical Data)

**R√¥le** : Datamart optimis√© pour requ√™tes OLAP

**Caract√©ristiques** :
- Format : 6 tables MySQL (5 dimensions + 1 fait)
- Sch√©ma : Star Schema (d√©normalis√© pour performance)
- Transformations : Normalisation dimensionnelle, lookup FK
- Dur√©e de vie : Persist√© (historisation future avec SCD)

**Architecture** :
- **Mod√®le** : Star Schema (√©toile simple, pas de flocon)
- **Granularit√© fait** : 1 snapshot nutritionnel par produit √ó date
- **Strat√©gie FK** : Lookup depuis Silver via jointures Spark
- **Indexes** : Sur toutes PK et FK

---

## ‚≠ê Sch√©ma de donn√©es Gold

### Diagramme entit√©-association

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     dim_time        ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ PK: time_sk         ‚îÇ‚óÑ‚îÄ‚îÄ‚îê
‚îÇ     date            ‚îÇ   ‚îÇ
‚îÇ     year, month...  ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    dim_brand        ‚îÇ   ‚îÇ   ‚îÇ  fact_nutrition_snapshot  ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ   ‚îÇ   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ PK: brand_sk        ‚îÇ‚óÑ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§ PK: fact_id               ‚îÇ
‚îÇ     brand_name      ‚îÇ   ‚îÇ   ‚îÇ FK: product_sk            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ FK: time_sk               ‚îÇ
                          ‚îÇ   ‚îÇ     energy_kcal_100g      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ     fat_100g, sugars...   ‚îÇ
‚îÇ   dim_category      ‚îÇ   ‚îÇ   ‚îÇ     nutriscore_grade      ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ PK: category_sk     ‚îÇ‚óÑ‚îÄ‚îÄ‚î§              ‚ñ≤
‚îÇ     category_name   ‚îÇ   ‚îÇ              ‚îÇ
‚îÇ     parent_cat_sk   ‚îÇ   ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ              ‚îÇ
                          ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ   dim_product      ‚îÇ
‚îÇ   dim_country       ‚îÇ   ‚îÇ   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ   ‚îÇ   ‚îÇ PK: product_sk     ‚îÇ
‚îÇ PK: country_sk      ‚îÇ   ‚îÇ   ‚îÇ     code           ‚îÇ
‚îÇ     countries_name  ‚îÇ   ‚îÇ   ‚îÇ FK: brand_sk       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ FK: primary_cat_sk ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚î§     product_name   ‚îÇ
                              ‚îÇ     countries...   ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Types de dimensions

| Dimension | Type SCD | Historisation | Justification |
|-----------|----------|---------------|---------------|
| dim_time | Type 0 (statique) | Non | Calendrier immuable |
| dim_brand | Type 1 (overwrite) | Non | Modifications marque rares |
| dim_category | Type 1 | Non | Taxonomie stable |
| dim_country | Type 1 | Non | G√©ographie statique |
| dim_product | Type 1 | **√Ä √©voluer vers Type 2** | Tracking changements produits |

**Recommandation future** : Impl√©menter SCD Type 2 sur `dim_product` avec colonnes :
- `valid_from` (DATE)
- `valid_to` (DATE, NULL = actif)
- `is_current` (BOOLEAN)

---

## üîÑ Strat√©gie de chargement

### Mode actuel : Full Overwrite

**Principe** : Remplacement complet des donn√©es √† chaque ex√©cution

**S√©quence** :
```sql
-- √âtape 1 : D√©sactiver contraintes FK (√©viter blocages)
SET FOREIGN_KEY_CHECKS = 0;

-- √âtape 2 : Truncate des tables (vidage)
TRUNCATE TABLE dim_time;
TRUNCATE TABLE dim_brand;
TRUNCATE TABLE dim_category;
TRUNCATE TABLE dim_country;
TRUNCATE TABLE dim_product;
TRUNCATE TABLE fact_nutrition_snapshot;

-- √âtape 3 : R√©activer contraintes
SET FOREIGN_KEY_CHECKS = 1;

-- √âtape 4 : Insertion via Spark JDBC (mode append)
spark_df.write.jdbc(..., mode="append")
```

**Avantages** :
- ‚úÖ Simplicit√© extr√™me (pas de gestion d'√©tat)
- ‚úÖ Garantie coh√©rence (snapshot complet)
- ‚úÖ Idempotence (relancer = m√™me r√©sultat)

**Inconv√©nients** :
- ‚ùå Perte d'historique (pas de versioning)
- ‚ùå Temps de traitement croissant (lin√©aire avec volume)
- ‚ùå Fen√™tre de downtime (tables vides pendant truncate)

---

### √âvolution future : Upsert incr√©mental

**Strat√©gie recommand√©e pour production** :

#### √âtape 1 : D√©tection des changements

**Source CDC** : API Open Food Facts (`/cgi/search.pl?last_modified_t>`)

```python
# R√©cup√©rer uniquement les produits modifi√©s depuis le dernier run
last_run_ts = get_last_watermark()  # Ex: 1640000000
new_products_df = spark.read.json(
    f"https://world.openfoodfacts.org/cgi/search.pl?json=1&last_modified_t>{last_run_ts}"
)
```

#### √âtape 2 : Upsert via MERGE (MySQL 8.0.19+)

```sql
-- Insertion/Mise √† jour dim_product
MERGE INTO dim_product AS target
USING staging_product AS source
ON target.code = source.code
WHEN MATCHED THEN
    UPDATE SET 
        product_name = source.product_name,
        brand_sk = source.brand_sk,
        modified_date = source.modified_date
WHEN NOT MATCHED THEN
    INSERT (code, product_name, brand_sk)
    VALUES (source.code, source.product_name, source.brand_sk);
```

**Alternative Spark** : Utiliser `.mode("overwrite")` avec condition :
```python
# Pseudo-code
existing_df = spark.read.jdbc(..., "dim_product")
new_df = silver_df.filter(col("last_modified_t") > watermark)

merged_df = existing_df.join(new_df, "code", "full_outer") \
    .select(coalesce(new_df.col, existing_df.col))

merged_df.write.jdbc(..., mode="overwrite")
```

#### √âtape 3 : SCD Type 2 sur dim_product

**Sch√©ma √©tendu** :
```sql
ALTER TABLE dim_product ADD COLUMN (
    valid_from DATE NOT NULL,
    valid_to DATE DEFAULT '9999-12-31',
    is_current BOOLEAN DEFAULT TRUE
);
```

**Logique d'upsert** :
```python
# Si le produit existe et a chang√©
if product_changed:
    # 1. Fermer l'ancienne version
    UPDATE dim_product 
    SET valid_to = CURRENT_DATE, is_current = FALSE
    WHERE code = ? AND is_current = TRUE
    
    # 2. Ins√©rer nouvelle version
    INSERT INTO dim_product (..., valid_from, is_current)
    VALUES (..., CURRENT_DATE, TRUE)
```

---

## ‚ö° Performance et scalabilit√©

### Optimisations impl√©ment√©es

#### 1. Partitionnement Spark

**Configuration locale** :
```python
spark = SparkSession.builder \
    .master("local[1]") \  # 1 seul worker (machine locale)
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()
```

**Configuration cluster (future)** :
```python
spark = SparkSession.builder \
    .master("spark://master:7077") \
    .config("spark.executor.instances", "4") \
    .config("spark.executor.cores", "4") \
    .config("spark.executor.memory", "8g") \
    .getOrCreate()
```

**Projection** : Avec 4 ex√©cuteurs, traitement estim√© √† 2 minutes (vs 8 actuellement)

---

## üìä Monitoring et observabilit√©

### M√©triques cl√©s

**Impl√©mentation** :
```python
# G√©n√©ration metrics_AAAAMMDD_HHMMSS.json
metrics = {
    "timestamp": datetime.now().isoformat(),
    "duree_minutes": (end_time - start_time).seconds / 60,
    "lignes_traitees": silver_final.count(),
    "lignes_rejetees": bronze_df.count() - silver_final.count(),
    "taux_completude_moyen": df_fact.agg({"completeness_score": "avg"}).first()[0],
    "status": "SUCCESS"
}
```